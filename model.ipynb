{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RN-WVtU5vAP0"
      },
      "source": [
        "## **Project2 - Neural networks for object detection**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "**Name:** Dana Haham\n",
        "\n",
        "**ID:** 209278407"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E3kDvvZHvUD3"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models.detection.mask_rcnn\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "from pycocotools.coco import COCO\n",
        "from pycocotools.cocoeval import COCOeval\n",
        "from pycocotools import mask as coco_mask\n",
        "\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "from PIL import Image, ImageDraw\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "!pip install roboflow\n",
        "from roboflow import Roboflow\n",
        "\n",
        "!pip install torchmetrics\n",
        "from torchmetrics.detection import MeanAveragePrecision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rm437U6ltNAG"
      },
      "outputs": [],
      "source": [
        "print(f\" is cuda available: {torch.cuda.is_available()}\")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r7NvIRilrhm-"
      },
      "outputs": [],
      "source": [
        "# Download dataset\n",
        "rf = Roboflow(api_key=\"xf1OzOc8OqX4qwcC4Jcf\")\n",
        "project = rf.workspace(\"drone-when8\").project(\"drone_bird_aircraft\")\n",
        "version = project.version(3)\n",
        "rf_dataset = version.download(\"coco-segmentation\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aGeNsVvIAOgG"
      },
      "outputs": [],
      "source": [
        "class CustomDataset(Dataset):\n",
        "\n",
        "    def __init__(self, root_dir, annotation_file, transforms=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transforms = transforms\n",
        "        self.coco = COCO(annotation_file)\n",
        "\n",
        "        # Initially get all image IDs\n",
        "        all_image_ids = self.coco.getImgIds()\n",
        "\n",
        "        # Filter out image IDs that have no annotations\n",
        "        self.image_ids = [img_id for img_id in all_image_ids if len(self.coco.getAnnIds(imgIds=img_id)) > 0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        # Loading image\n",
        "        img_id = self.image_ids[idx]\n",
        "        img_info = self.coco.loadImgs(img_id)[0]\n",
        "        img_path = os.path.join(self.root_dir, img_info['file_name'])\n",
        "        image = cv2.imread(img_path)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # Loading annotation\n",
        "        ann_ids = self.coco.getAnnIds(imgIds=img_id)\n",
        "        coco_annotations = self.coco.loadAnns(ann_ids)\n",
        "\n",
        "        # Create arrays with compatible format for the model\n",
        "        bboxes = []\n",
        "        labels = []\n",
        "        masks = []\n",
        "\n",
        "        for ann in coco_annotations:\n",
        "\n",
        "          # Convert COCO bbox format from [x, y, width, height] to [x_min, y_min, x_max, y_max]\n",
        "          bbox = ann['bbox']\n",
        "          bbox = [bbox[0], bbox[1], min(bbox[0] + bbox[2], img_info['width']), min(bbox[1] + bbox[3], img_info['height'])]\n",
        "\n",
        "          # Convert COCO segmentation format from polygon to binary mask\n",
        "          segmentation = ann['segmentation'][0]\n",
        "          polygon = np.array(segmentation).reshape((-1, 2))\n",
        "          mask = np.zeros((img_info['height'], img_info['width']), dtype=np.uint8)\n",
        "          cv2.fillPoly(mask, [polygon.astype(np.int32)], 1)\n",
        "\n",
        "          bboxes.append(bbox)\n",
        "          masks.append(mask)\n",
        "          labels.append(ann['category_id'])\n",
        "\n",
        "        masks = np.stack(masks, axis=0)\n",
        "\n",
        "        # Apply transformations\n",
        "        if self.transforms:\n",
        "            transformed = self.transforms(image=image, bboxes=bboxes, category_id=labels, segmentation=masks)\n",
        "            image = transformed['image']\n",
        "            bboxes = transformed['bboxes'] if transformed['bboxes'] else [[0, 0, 1e-5, 1e-5]]\n",
        "            labels = transformed['category_id']\n",
        "            masks = transformed['segmentation']\n",
        "\n",
        "        bboxes = [bbox if (bbox[2] > 0 and bbox[3] > 0) else [0,0, 1e-5, 1e-5] for bbox in bboxes]\n",
        "\n",
        "        # Convert everything into torch tensors\n",
        "        targets = {\n",
        "        'boxes': torch.as_tensor(bboxes, dtype=torch.float32),\n",
        "        'labels': torch.as_tensor(labels, dtype=torch.int64),\n",
        "        'masks': torch.as_tensor(masks, dtype=torch.uint8),\n",
        "        'image_id': torch.tensor([img_id])\n",
        "         }\n",
        "\n",
        "        return image, targets\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pwlkyuptyWSR"
      },
      "outputs": [],
      "source": [
        "# Pre-processing the datasets\n",
        "train_transform = A.Compose([\n",
        "\n",
        "    # Geometric transforms\n",
        "    A.Resize(225, 225),\n",
        "    A.HorizontalFlip(p=0.5),\n",
        "    A.RandomRotate90(p=0.5),\n",
        "\n",
        "    # Color and noise transforms\n",
        "    A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2, p=0.3),\n",
        "    A.GaussNoise(var_limit=(10.0, 50.0), p=0.5),\n",
        "\n",
        "    # Normalization and conversion to tensor\n",
        "    A.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n",
        "    ToTensorV2()\n",
        "], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['category_id']))\n",
        "\n",
        "test_transform = A.Compose([\n",
        "    A.Resize(225, 225),\n",
        "    A.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n",
        "    ToTensorV2()\n",
        "], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['category_id']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A0VV1KT59LED"
      },
      "outputs": [],
      "source": [
        "# Train data path\n",
        "train_path_to_images = os.path.join(rf_dataset.location, \"train\")\n",
        "train_path_to_annotations = os.path.join(train_path_to_images, \"_annotations.coco.json\")\n",
        "\n",
        "# Validation data path\n",
        "valid_path_to_images = os.path.join(rf_dataset.location, \"valid\")\n",
        "valid_path_to_annotations = os.path.join(valid_path_to_images, \"_annotations.coco.json\")\n",
        "\n",
        "# Test data path\n",
        "test_path_to_images = os.path.join(rf_dataset.location, \"test\")\n",
        "test_path_to_annotations = os.path.join(test_path_to_images, \"_annotations.coco.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gKod-O1r-EJW"
      },
      "outputs": [],
      "source": [
        "# Create datasets\n",
        "train_dataset = CustomDataset(root_dir = train_path_to_images,\n",
        "                             annotation_file = train_path_to_annotations,\n",
        "                             transforms = train_transform)\n",
        "\n",
        "valid_dataset = CustomDataset(root_dir = valid_path_to_images,\n",
        "                             annotation_file = valid_path_to_annotations,\n",
        "                             transforms = test_transform)\n",
        "\n",
        "test_dataset = CustomDataset(root_dir = test_path_to_images,\n",
        "                             annotation_file = test_path_to_annotations,\n",
        "                             transforms = test_transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mSll4Io34qvs"
      },
      "outputs": [],
      "source": [
        "# Load the datasets\n",
        "train_loader = DataLoader(train_dataset, batch_size=160, shuffle=True, collate_fn=lambda x: tuple(zip(*x)), num_workers=2, pin_memory=True)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=160, shuffle=False, collate_fn=lambda x: tuple(zip(*x)), num_workers=2, pin_memory=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=160, shuffle=False, collate_fn=lambda x: tuple(zip(*x)), num_workers=2, pin_memory=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N3cXAAmPOhA-"
      },
      "outputs": [],
      "source": [
        "# Bird, drone, aircraft and background\n",
        "num_classes = 4\n",
        "\n",
        "# Transfer Learning on MASK RCNN with Fine-Tuning\n",
        "\n",
        "# Load Pretrained Mask R-CNN:\n",
        "model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n",
        "\n",
        "# Freeze all the layers of the model\n",
        "for param in model.backbone.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Unfreeze the classifier and box predictor in the ROI heads\n",
        "for param in model.roi_heads.box_predictor.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "# Unfreeze the mask predictor\n",
        "for param in model.roi_heads.mask_predictor.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "# Modify the model\n",
        "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "\n",
        "in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
        "hidden_layer = 256\n",
        "model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask, hidden_layer, num_classes)\n",
        "\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0XUCwT8etKZU"
      },
      "outputs": [],
      "source": [
        "def valid_loss():\n",
        "\n",
        "    # The way to run loss on validation\n",
        "    model.train()\n",
        "\n",
        "    val_loss_sum = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        # Run mini-batch\n",
        "        for images, targets in valid_loader:\n",
        "\n",
        "            # Move batch to device\n",
        "            images = list(img.to(device) for img in images)\n",
        "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "            # Sum all the losses in the dictionary\n",
        "            loss_dict = model(images, targets)\n",
        "\n",
        "            losses = sum(loss for loss in loss_dict.values())\n",
        "            val_loss_sum += losses.item()\n",
        "\n",
        "    # Calculate the loss avarage\n",
        "    val_loss = val_loss_sum / len(valid_loader)\n",
        "\n",
        "    return val_loss"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(data_loader):\n",
        "\n",
        "    # Initialize the metric\n",
        "    map_metric = MeanAveragePrecision(box_format='xyxy', iou_thresholds=[0.5], class_metrics=True)\n",
        "\n",
        "    # Set the model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        # Run mini-batch\n",
        "        for images, targets in data_loader:\n",
        "\n",
        "            # Move batch to device\n",
        "            images = list(img.to(device) for img in images)\n",
        "\n",
        "            # Get predictions in the model format\n",
        "            outputs = model(images)\n",
        "\n",
        "            # Prepare data for MeanAveragePrecision\n",
        "            preds = []\n",
        "            for output in outputs:\n",
        "                pred_boxes = output['boxes'].cpu()\n",
        "                pred_scores = output['scores'].cpu()\n",
        "                pred_labels = output['labels'].cpu()\n",
        "                preds.append({\n",
        "                    'boxes': pred_boxes,\n",
        "                    'scores': pred_scores,\n",
        "                    'labels': pred_labels\n",
        "                })\n",
        "\n",
        "            gts = []\n",
        "            for target in targets:\n",
        "                gt_boxes = target['boxes']\n",
        "                gt_labels = target['labels']\n",
        "                gts.append({\n",
        "                    'boxes': gt_boxes,\n",
        "                    'labels': gt_labels\n",
        "                })\n",
        "\n",
        "            # Update metric for current batch\n",
        "            map_metric.update(preds, gts)\n",
        "\n",
        "    # Compute final mAP\n",
        "    final_map = map_metric.compute()\n",
        "    print(f\"Mean Average Precision: {final_map}\")\n",
        "\n",
        "    return final_map.get(\"map\")"
      ],
      "metadata": {
        "id": "kiF-_G1gYB5U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4RzlMFyyoTM_"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir logs\n",
        "\n",
        "writer = SummaryWriter('logs')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mgn7kY_haT31"
      },
      "outputs": [],
      "source": [
        "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
        "\n",
        "# Traning model\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size= 10, gamma=0.001)\n",
        "best_val_loss = float('inf')\n",
        "\n",
        "epochs = 13\n",
        "for e in range(epochs):\n",
        "\n",
        "    # Set the model to tarin mode\n",
        "    model.train()\n",
        "    epoch_sum_loss = 0\n",
        "\n",
        "    # Run mini-batch\n",
        "    for images, targets in train_loader:\n",
        "\n",
        "        # Move batch to daeice\n",
        "        images = list(img.to(device) for img in images)\n",
        "        targets = [{k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in t.items()} for t in targets]\n",
        "\n",
        "        # Training pass\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Sum all the losses in the dictionary\n",
        "        loss_dict = model(images, targets)\n",
        "        losses = sum(loss for loss in loss_dict.values())\n",
        "\n",
        "        # The model learns by backpropagating\n",
        "        losses.backward()\n",
        "\n",
        "        # Optimizes its weights\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_sum_loss += losses.item()\n",
        "\n",
        "    # Improve model\n",
        "    scheduler.step()\n",
        "    lr = optimizer.param_groups[0]['lr']\n",
        "\n",
        "    # Evaluation\n",
        "    train_loss = epoch_sum_loss/len(train_loader)\n",
        "    val_loss = valid_loss()\n",
        "    val_acc = accuracy(valid_loader)\n",
        "\n",
        "    # Find the model with the best validation loss\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "\n",
        "        # Save the model in the drive\n",
        "        drive.mount('/content/drive', force_remount=True)\n",
        "        model_save_path = '/content/drive/MyDrive/Project2/model.pth'\n",
        "        torch.save(model.state_dict(), model_save_path)\n",
        "\n",
        "    # Visualizing with TensorBoard\n",
        "    writer.add_scalar('Loss/train', train_loss, e)\n",
        "    writer.add_scalar('Loss/val', val_loss, e)\n",
        "    writer.add_scalar('Accuracy/val', val_acc, e)\n",
        "    writer.add_scalar('Learning Rate', lr, e)\n",
        "\n",
        "    print(e)\n",
        "\n",
        "    print(f\"Epoch {e}, train_loss {train_loss:.3f}, val_loss {val_loss:.3f}\")\n",
        "\n",
        "writer.flush()\n",
        "writer.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pFD2SZae_Ylb"
      },
      "outputs": [],
      "source": [
        "# Test model\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# Load trained model\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/Project2/model.pth'))\n",
        "\n",
        "accuracy(test_loader)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}