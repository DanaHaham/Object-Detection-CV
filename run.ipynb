{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **Project2 - Neural networks for object detection**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "**Name:** Dana Haham\n",
        "\n",
        "**ID:** 209278407"
      ],
      "metadata": {
        "id": "RN-WVtU5vAP0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "import sys\n",
        "import shutil\n",
        "import torch\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models.detection.mask_rcnn\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
        "\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "!pip install pytube\n",
        "from pytube import YouTube"
      ],
      "metadata": {
        "id": "TRzlnxCPn4HL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\" is cuda available: {torch.cuda.is_available()}\")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "WUHg7n9WgHmU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dict of the categories in the trained dataset\n",
        "class_names = {\n",
        "  0: 'drone-bird-aircraft',\n",
        "  1: 'aircraft',\n",
        "  2: 'bird',\n",
        "  3: 'drone'\n",
        "}\n",
        "\n",
        "# Pre-processing the image\n",
        "img_transform = A.Compose([\n",
        "    A.Resize(225, 225),\n",
        "    A.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n",
        "    ToTensorV2()\n",
        "])"
      ],
      "metadata": {
        "id": "TCKIdpSMu3MB"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_objects(image, detected_frame, model):\n",
        "\n",
        "  # Set the model to evaluation mode\n",
        "  model.eval()\n",
        "\n",
        "  # Apply transformations to the image\n",
        "  transformed = img_transform(image=image)\n",
        "  transformed_image = transformed['image']\n",
        "\n",
        "  # Add a batch dimension (BxCxHxW)\n",
        "  transformed_image = transformed_image.unsqueeze(0)\n",
        "\n",
        "  # Move the image tensor to the same device as the model\n",
        "  transformed_image = transformed_image.to(device)\n",
        "\n",
        "  # Perform inference\n",
        "  with torch.no_grad():\n",
        "    predictions = model(transformed_image)\n",
        "\n",
        "  # Get the predicted class and bounding box\n",
        "  pred_boxes = predictions[0]['boxes'].detach().cpu().numpy()\n",
        "  pred_labels = predictions[0]['labels'].detach().cpu().numpy()\n",
        "  pred_scores = predictions[0]['scores'].detach().cpu().numpy()\n",
        "\n",
        "  # Filter out predictions with low confidence\n",
        "  CONFIDENCE_THRESHOLD = 0.85\n",
        "  high_confidence_idxs = pred_scores > CONFIDENCE_THRESHOLD\n",
        "\n",
        "  pred_boxes = pred_boxes[high_confidence_idxs]\n",
        "  pred_labels = pred_labels[high_confidence_idxs]\n",
        "  pred_scores = pred_scores[high_confidence_idxs]\n",
        "\n",
        "  # Get the original image dimensions\n",
        "  height, width = image.shape[:2]\n",
        "\n",
        "  scale_x = width / 225\n",
        "  scale_y = height / 225\n",
        "\n",
        "  for box, label, score in zip(pred_boxes, pred_labels, pred_scores):\n",
        "\n",
        "        # Calculate bounding box coordinates\n",
        "        x_min = int(box[0] * scale_x)\n",
        "        y_min = int(box[1] * scale_y)\n",
        "        x_max = int(box[2] * scale_x)\n",
        "        y_max = int(box[3] *scale_y)\n",
        "\n",
        "        # Draw the bounding box\n",
        "        cv2.rectangle(detected_frame, (x_min, y_min), (x_max, y_max), (255, 0, 0), 4)\n",
        "\n",
        "        # Add class name label\n",
        "        class_name = class_names[label]\n",
        "        cv2.putText(detected_frame, class_name, (x_min, y_min - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
        "\n",
        "  return detected_frame"
      ],
      "metadata": {
        "id": "tTrd00biSYHb"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model():\n",
        "\n",
        "  # Bird, drone, aircraft and background\n",
        "  num_classes = 4\n",
        "\n",
        "  # Transfer Learning on MASK RCNN with Fine-Tuning\n",
        "\n",
        "  # Load Pretrained Mask R-CNN:\n",
        "  model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n",
        "\n",
        "  # Freeze all the layers of the model\n",
        "  for param in model.backbone.parameters():\n",
        "      param.requires_grad = False\n",
        "\n",
        "  # Unfreeze the classifier and box predictor in the ROI heads\n",
        "  for param in model.roi_heads.box_predictor.parameters():\n",
        "      param.requires_grad = True\n",
        "\n",
        "  # Unfreeze the mask predictor\n",
        "  for param in model.roi_heads.mask_predictor.parameters():\n",
        "      param.requires_grad = True\n",
        "\n",
        "  # Modify the model\n",
        "  in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "  model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "\n",
        "  in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
        "  hidden_layer = 256\n",
        "  model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask, hidden_layer, num_classes)\n",
        "\n",
        "  model.to(device)\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "n22qU2jef7je"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Open video in the given path\n",
        "def open_video(video_path):\n",
        "\n",
        "    # Open the video file\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "    # Check if the video was successfully loaded\n",
        "    if not cap.isOpened():\n",
        "        print(\"Error: Could not open video.\")\n",
        "        exit()\n",
        "\n",
        "    return cap\n",
        "\n",
        "# Create new video in the given path acordding to the given properties\n",
        "def create_video(output_path, fps, frame_width, frame_height):\n",
        "\n",
        "    # Create the video file\n",
        "    write = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (frame_width, frame_height))\n",
        "\n",
        "    return write\n",
        "\n",
        "# Release the given videos\n",
        "def release_videos(video_input, video_output):\n",
        "    video_input.release()\n",
        "    video_output.release()\n",
        "\n",
        "# Download the video from the given url to colab\n",
        "def download_youtube_video(url):\n",
        "\n",
        "  # Colab's working directory\n",
        "  download_path = '/content'\n",
        "  yt = YouTube(url)\n",
        "\n",
        "  # Download the highest resolution video\n",
        "  yt.streams.filter(progressive=True, file_extension='mp4').order_by('resolution').desc().first().download(output_path=download_path, filename=\"input.mp4\")\n",
        "\n",
        "# Download the given video to the drive\n",
        "def download_output():\n",
        "  drive.mount('/content/drive', force_remount=True)\n",
        "  output_directory = '/content/drive/MyDrive/Project2'\n",
        "\n",
        "  # Ensure the output directory exists\n",
        "  os.makedirs(output_directory, exist_ok=True)\n",
        "\n",
        "  # Move the file using shutil for handling file operations\n",
        "  shutil.move(\"/content/output.mp4\", os.path.join(output_directory, \"output.mp4\"))\n",
        "\n",
        "# Load the best model\n",
        "def load_model(model_path):\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "    # Create model archicture\n",
        "    model = create_model()\n",
        "\n",
        "    # Load trained model\n",
        "    model.load_state_dict(torch.load(model_path))\n",
        "    model.cuda()\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "ArCxQh3kXkTq"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the algorithm on the given video\n",
        "def handle_video():\n",
        "\n",
        "    # Download the video from youtube to colab\n",
        "    video_url = 'https://www.youtube.com/watch?v=hoHAC2b1K0Q'\n",
        "    download_youtube_video(video_url)\n",
        "\n",
        "    # Open the video file\n",
        "    og_video = open_video('/content/input.mp4')\n",
        "\n",
        "    # Load the model\n",
        "    model = load_model('/content/drive/MyDrive/Project2/model.pth')\n",
        "\n",
        "    # Read first frame\n",
        "    ret, frame = og_video.read()\n",
        "\n",
        "    # Define the output video writer\n",
        "    detected_video = create_video(f'/content/output.mp4', og_video.get(cv2.CAP_PROP_FPS), int(og_video.get(cv2.CAP_PROP_FRAME_WIDTH)), int(og_video.get(cv2.CAP_PROP_FRAME_HEIGHT)))\n",
        "\n",
        "    # Capture farme by frame until the video is over\n",
        "    while(ret):\n",
        "\n",
        "        # Create new image for the detected objects in the frame\n",
        "        detected_frame = np.zeros_like(frame)\n",
        "\n",
        "        # Detect the object in the frame\n",
        "        detected_frame = detect_objects(frame, detected_frame, model)\n",
        "\n",
        "        # Blend the detected objects in the frame\n",
        "        res_image = cv2.addWeighted(frame, 0.8 ,detected_frame, 1, 0)\n",
        "\n",
        "        # Add the detected frame to the output video\n",
        "        detected_video.write(res_image)\n",
        "\n",
        "        # Continue to the next frame\n",
        "        ret, frame = og_video.read()\n",
        "\n",
        "    # Release the video\n",
        "    release_videos(og_video, detected_video)\n",
        "\n",
        "    # Download the video to the drive\n",
        "    download_output()"
      ],
      "metadata": {
        "id": "_b-F2ZJxx58a"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "  # Active the actions on the video\n",
        "  handle_video()"
      ],
      "metadata": {
        "id": "MQApXJp-ytKp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}